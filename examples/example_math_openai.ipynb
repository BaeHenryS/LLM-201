{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Generation, Fine-Tuning, and Evaluation\n",
    "\n",
    "Example Notebook outlining the steps to load dataset, fine-tune a model on OpenAI, and evaluate it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and Install Packages\n",
    "!pip install python-dotenv\n",
    "!pip install openai\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/BaeHenryS/LLM-201.git\n",
    "    %cd LLM-201"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Option 1:**\n",
    "\n",
    "To keep track of problems easily, one would ideally use a spreadsheet to organize the problems. The spreadsheet can be exported to a .csv file. Here is a [Google sheet example](https://docs.google.com/spreadsheets/d/18NZErrCDF55uFSx_AjWUba93NSVDs4yHGcM9ZxiFRf4/edit#gid=0) for the .csv format.\n",
    "\n",
    "Each problem (row) should contain the following information (columns):\n",
    "\n",
    "| Section            | Description           |\n",
    "| ------------------ | --------------------- |\n",
    "| problem            | The statement of the math problem |\n",
    "| type               | The category of the problem (e.g. AM201) |\n",
    "| level              | The difficulty level of the problem in integer (optional) |\n",
    "| solution           | The solution to the problem, the final solution should be withinin \\$\\\\\\boxed{}\\$ |\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2:**\n",
    "\n",
    "One could alternatively follow the .json/.jsonl format. with the following format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"problem\": \"The statement of the math problem\",\n",
    "    \"type\": \"The category of the problem (e.g. AM201)\",\n",
    "    \"level\": \"The difficulty level of the problem in integer (optional)\",\n",
    "    \"solution\": \"The solution to the problem, the final solution should be withinin \\$\\\\\\boxed{}\\$\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: MATH Dataset\n",
    "\n",
    "For the MATH Dataset, note that one needs to combine the individual .json files into a .jsonl file (standard formatting used for inference and fine tuning). The cell below uses the algebra problems in Math Dataset as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_json_to_jsonl(folder_path, output_file):\n",
    "    \"\"\"\n",
    "    Concatenates multiple JSON files into a single JSONL file.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing the JSON files.\n",
    "        output_file (str): The path to the output JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for file_name in json_files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            with open(file_path, 'r') as infile:\n",
    "                data = json.load(infile)\n",
    "                json.dump(data, outfile)\n",
    "                outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now obtain the MATH Dataset. You can change the subject. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'algebra'  # Replace with the subject you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directory depends on the whether we are working at Google Colab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting ... \n",
      "Local Repository Detected... \n"
     ]
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Converting ... \")\n",
    "    train_folder_path = f'./MATH_Dataset/MATH/train/{subject}/' \n",
    "    test_output_file = f'./examples/data/{subject}_train.jsonl'  \n",
    "    concatenate_json_to_jsonl(train_folder_path, test_output_file)\n",
    "\n",
    "    test_folder_path = f'./MATH_Dataset/MATH/test/{subject}/'  \n",
    "    test_output_file = f'./examples/data/{subject}_test.jsonl'  \n",
    "    concatenate_json_to_jsonl(test_folder_path, test_output_file)\n",
    "else:\n",
    "    print(\"Local Repository Detected... \")\n",
    "    train_folder_path = f'../MATH_Dataset/MATH/train/{subject}/' \n",
    "    test_output_file = f'./data/{subject}_train.jsonl'  \n",
    "    concatenate_json_to_jsonl(train_folder_path, test_output_file)\n",
    "\n",
    "    test_folder_path = f'../MATH_Dataset/MATH/test/{subject}/'  \n",
    "    test_output_file = f'./data/{subject}_test.jsonl'  \n",
    "    concatenate_json_to_jsonl(test_folder_path, test_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up API for LLMs (focus on OpenAI models for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the models provided by OpenAI, we first need to set up access to the OpenAI API. For this step, we can follow the [instructions provided by OpenAI](https://platform.openai.com/docs/quickstart?context=python). Test that the API is working by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='In the realm where code does dance and spin,\\nResides a mystical power, deep within.\\nIt\\'s called the art of recursion, dear friend,\\nA concept that stretches, without an end.\\n\\nPicture a waltz, a dance of great grace,\\nWhere steps repeat, in an elegant chase.\\nRecursion, too, is a dance to behold,\\nAs functions call themselves, in a story untold.\\n\\nFrom a start so humble, a function is born,\\nDelving into mysteries, yet to be drawn.\\nIt takes a problem, too complex to unwind,\\nAnd breaks it into parts, as if divinely assigned.\\n\\nWith a call to itself, it tumbles through space,\\nUnraveling layers, at a fervent pace.\\nLike a tapestry woven, stitch by stitch,\\nRecursion unveils the answer, without a glitch.\\n\\nImagine a forest, vast and profound,\\nEach tree branching out, with wonders abound.\\nRecursion, too, traverses this domain,\\nExploring depths, a mesmerizing refrain.\\n\\nA function calls itself, with confidence and might,\\nPeeling away layers, like an explorer at night.\\nEach call creates a new branch, a path to explore,\\nUntil a base case is reached, and it cries \"no more!\"\\n\\nLike a puzzle unraveling, piece by piece,\\nRecursion brings order, granting us peace.\\nFor in this dance of functions, there\\'s beauty untold,\\nA captivating symphony, full of stories to unfold.\\n\\nSo embrace this enchantment, dear coder in line,\\nLet recursion be your muse, both humble and divine.\\nFor in its embrace, you\\'ll find beauty and grace,\\nAs you dance through the realms, in this coding maze.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Write your API Key Here\n",
    "api_key = \"YOUR_API_KEY\"\n",
    "\n",
    "# You can also set your API key via the OPENAI_API_KEY environment variable\n",
    "# load_dotenv()\n",
    "# openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "# Test the API\n",
    "client = openai.Client(api_key = api_key)\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Customized Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below extracts the solutions from the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Directory Based on Local or Colab\n",
    "\n",
    "if IN_COLAB:\n",
    "    %cd examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for extracting boxed value\n",
    "def extract_number_from_latex(paragraph):\n",
    "    \"\"\"\n",
    "    Extracts a number from a LaTeX paragraph enclosed in the '$\\boxed{}$' pattern.\n",
    "    \n",
    "    Parameters:\n",
    "        paragraph (str): The LaTeX paragraph containing the number enclosed in the '$\\boxed{}$' pattern.\n",
    "        \n",
    "    Returns:\n",
    "        int or None: The extracted number if found, otherwise None.\n",
    "    \"\"\"\n",
    "    # Regular expression to find the $\\boxed{x}$ pattern\n",
    "    # This assumes 'x' is an integer; modify the regex if 'x' can be a different type of number\n",
    "    match = re.search(r'\\$\\\\boxed\\{(\\d+)\\}', paragraph)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calls OpenAI API to generate solutions for the problems in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def math_openai_call(model, jsonl_line):\n",
    "    \"\"\"\n",
    "    Given a model and a line from the customized dataset, output side-by-side the model solution and the target solution to the nth example.\n",
    "    \n",
    "    Parameters:\n",
    "        n (int): n-th line of the .jsonl file.\n",
    "        model (str): a version of chatgpt model.\n",
    "        jsonl_line (str): a line from the customized dataset.\n",
    "        \n",
    "    Returns:\n",
    "        json_data (dict): The original data from the jsonl_line with the added AI solution.\n",
    "    \"\"\"\n",
    "    json_data = json.loads(jsonl_line)\n",
    "    problem = json_data['problem']\n",
    "    solution = json_data['solution']\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant, skilled in solving mathematics problems. Solve this math problem by the user and return the answer (a number) in latex box format, e.g. $\\boxed$:\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{problem}\"},\n",
    "        ],\n",
    "        temperature=1,\n",
    "        max_tokens=256,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    ai_solution = response.choices[0].message.content\n",
    "    json_data['ai_solution'] = ai_solution\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the generated solutions with the ground truth solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for comparing values\n",
    "def compared_boxed_val(paragraph1, paragraph2):\n",
    "    \"\"\"\n",
    "    Given 2 paragraphs, return whether they contain the same boxed value. This function is used to check the ai solution against target solution.\n",
    "    \n",
    "    Parameters:\n",
    "        paragraph1 (str)\n",
    "        paragraph2 (str)\n",
    "        \n",
    "    Returns:\n",
    "        Boolean: True if two paragraphs contain the same boxed value and False otherwise\n",
    "    \"\"\"\n",
    "    num1 = extract_number_from_latex(paragraph1)\n",
    "    num2 = extract_number_from_latex(paragraph2)\n",
    "    return num1 == num2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to test a batch of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def eval_math_problems(input_jsonl_path, output_jsonl_path, model, num_lines_to_parse):\n",
    "    \"\"\"\n",
    "    Test a batch of math problems using the given model.\n",
    "\n",
    "    Args:\n",
    "        input_jsonl_path (str): The path to the input JSONL file containing math problems.\n",
    "        output_jsonl_path (str): The path to the output JSONL file to store the results.\n",
    "        model: The model used to solve the math problems.\n",
    "        num_lines_to_parse (int): The number of lines to parse from the input file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the test score (number of correct answers) and the total count of problems tested.\n",
    "    \"\"\"\n",
    "    test_score = 0\n",
    "    total_count = 0\n",
    "\n",
    "    # Check if output file exists, if not, create it\n",
    "    if not os.path.isfile(output_jsonl_path):\n",
    "        open(output_jsonl_path, 'w').close()\n",
    "\n",
    "    with open(input_jsonl_path, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            # Find which line we are at with output_jsonl_path:\n",
    "            checkpoint = 0\n",
    "            with open(output_jsonl_path, 'r') as outfile:\n",
    "                for j, _ in enumerate(outfile):  # Changed variable name here\n",
    "                    checkpoint += 1\n",
    "            if i < num_lines_to_parse and i >= checkpoint:\n",
    "                json_data = math_openai_call(model, line)\n",
    "                ai_solution = json_data['ai_solution']\n",
    "                solution = json_data['solution']\n",
    "                correct = compared_boxed_val(solution, ai_solution)\n",
    "                json_data['correct'] = correct\n",
    "                if correct:\n",
    "                    test_score += 1\n",
    "                total_count += 1 \n",
    "                with open(output_jsonl_path, 'a') as outfile:\n",
    "                    json.dump(json_data, outfile)\n",
    "                    outfile.write('\\n')\n",
    "                print(f\"Line {i+1} completed.\")\n",
    "            else:\n",
    "                if total_count != 0:\n",
    "                    print(f\"Test score: {test_score/total_count}\")\n",
    "                break\n",
    "    return test_score, total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "eval_math_problems('./data/algebra_train.jsonl', 'algebra_train_result.jsonl', \"gpt-3.5-turbo-1106\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning with Generated Dataaset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the OpenAI API to fine tune GPT-3.5 with the generated dataset. To do so, we first convert the dataset into a .jsonl file format accepted by the OpenAI API. The function below converts the dataset into the system, user, and assistant format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_format(jsonl_line):\n",
    "    \"\"\"\n",
    "    Given a line from the customized dataset, convert it to the OpenAI Chat format.\n",
    "    \n",
    "    Parameters:\n",
    "        jsonl_line (str): a line from the customized dataset.\n",
    "        \n",
    "    Returns:\n",
    "        new_format (str): The converted line in the OpenAI Chat format.\n",
    "    \"\"\"\n",
    "    json_data = json.loads(jsonl_line)\n",
    "    problem = json_data['problem']\n",
    "    solution = json_data['solution']\n",
    "    \n",
    "    new_format = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant, skilled in solving mathematics problems. Solve this math problem by the user and return the answer (a number) in latex box format, e.g. $\\\\boxed$:\"},\n",
    "            {\"role\": \"user\", \"content\": problem},\n",
    "            {\"role\": \"assistant\", \"content\": solution}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return json.dumps(new_format)\n",
    "\n",
    "def convert_file(input_filename, output_filename):\n",
    "    with open(input_filename, 'r') as input_file, open(output_filename, 'w') as output_file:\n",
    "        for line in input_file:\n",
    "            converted_line = convert_format(line)\n",
    "            output_file.write(converted_line + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the dataset into the system, user, and assistant format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_file('./data/algebra_train.jsonl', './data/algebra_train_openai.jsonl')\n",
    "convert_file('./data/algebra_test.jsonl', './data/algebra_test_openai.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the train and test dataset to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_create = client.files.create(\n",
    "  file=open(\"./data/algebra_train_openai.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "val_create = client.files.create(\n",
    "  file=open(\"./data/algebra_test_openai.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get ID of training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('file-MnyaGU86dQGlERAequXA3yHl', 'file-R21MMQOlvPz1LXWqcRMX0OaO')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_file = train_create.id\n",
    "val_file = val_create.id\n",
    "training_file, val_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the fine-tuning job. Now, go to the [OpenAI fine-tuning page](https://platform.openai.com/finetune/), and you should see the fine-tuning job running. After the fine tuning has been completed, you can copy the name of the fine tuned model, which will be used in the `model` parameter in the evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-94ECAxMcgCiNTl16Ao1k6un4', created_at=1702848382, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-1106', object='fine_tuning.job', organization_id='org-Lepq4jszFhmPPFz2pwTV628o', result_files=[], status='validating_files', trained_tokens=None, training_file='file-MnyaGU86dQGlERAequXA3yHl', validation_file=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_create = client.fine_tuning.jobs.create(\n",
    "  training_file=training_file, \n",
    "  model=\"gpt-3.5-turbo-1106\",\n",
    "  epochs=3,\n",
    "  # validation_file=val_file,\n",
    ")\n",
    "ft_create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ftjob-94ECAxMcgCiNTl16Ao1k6un4'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_job = ft_create.id\n",
    "ft_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Performance of Fine-Tuned Model with Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1 completed.\n",
      "Test score: 1.0\n",
      "Original Train Result: (1, 1)\n",
      "Original Test Result: (0, 0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Create a folder called \"results\"\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "def calculate_accuracy(result):\n",
    "    a, b = result\n",
    "    return a / b if b != 0 else 0\n",
    "\n",
    "\n",
    "# Original Model Result\n",
    "train_original_result = eval_math_problems('./data/algebra_train.jsonl', './results/algebra_train_result.jsonl', \"gpt-3.5-turbo-1106\", 1)\n",
    "test_original_result = eval_math_problems('./data/algebra_test.jsonl', './results/algebra_test_result.jsonl', \"gpt-3.5-turbo-1106\", 1)\n",
    "\n",
    "print(f\"Original Train Result: {calculate_accuracy(train_original_result)}\")\n",
    "print(f\"Original Test Result: {calculate_accuracy(test_original_result)}\")\n",
    "\n",
    "# Fine-tuned Model Result\n",
    "train_ft_result = eval_math_problems('./data/algebra_train.jsonl', './results/algebra_train_ft_result.jsonl', ft_job, 1)\n",
    "test_ft_result = eval_math_problems('./data/algebra_test.jsonl', './results/algebra_test_ft_result.jsonl', ft_job, 1)\n",
    "\n",
    "print(f\"Fine-tuned Train Result: {calculate_accuracy(train_ft_result)}\")\n",
    "print(f\"Fine-tuned Test Result: {calculate_accuracy(test_ft_result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
