{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Generation, Fine-Tuning, and Evaluation\n",
    "\n",
    "Example Notebook outlining the steps to load dataset, fine-tune a model on OpenAI, and evaluate it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and Install Packages\n",
    "#!pip install python-dotenv\n",
    "#!pip install openai\n",
    "import csv\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/BaeHenryS/LLM-201.git\n",
    "    %cd LLM-201"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Option 1:**\n",
    "\n",
    "To keep track of problems easily, one would ideally use a spreadsheet to organize the problems. The spreadsheet can be exported to a .csv file. Here is a [Google sheet example](https://docs.google.com/spreadsheets/d/18NZErrCDF55uFSx_AjWUba93NSVDs4yHGcM9ZxiFRf4/edit#gid=0) for the .csv format.\n",
    "\n",
    "Each problem (row) should contain the following information (columns):\n",
    "\n",
    "| Section            | Description           |\n",
    "| ------------------ | --------------------- |\n",
    "| problem            | The statement of the math problem |\n",
    "| type               | The category of the problem (e.g. AM201) |\n",
    "| level              | The difficulty level of the problem in integer (optional) |\n",
    "| solution           | The solution to the problem, the final solution should be withinin \\$\\\\\\boxed{}\\$ |\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2:**\n",
    "\n",
    "One could alternatively follow the .json/.jsonl format. with the following format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"problem\": \"The statement of the math problem\",\n",
    "    \"type\": \"The category of the problem (e.g. AM201)\",\n",
    "    \"level\": \"The difficulty level of the problem in integer (optional)\",\n",
    "    \"solution\": \"The solution to the problem, the final solution should be withinin \\$\\\\\\boxed{}\\$\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: AM201 Dataset\n",
    "\n",
    "For the AM201 Dataset, it is collected in the Google Sheet and converted to the format of .csv. The cell below provides a example of converting a .csv file into the .jsonl file suitable for evaluation and fine tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write rows to a JSONL file\n",
    "def write_to_jsonl(file_path, rows):\n",
    "    with open(file_path, mode='w', encoding='utf-8') as file:\n",
    "        for row in rows:\n",
    "            json_line = json.dumps(row)\n",
    "            file.write(json_line + '\\n')\n",
    "\n",
    "# Writing to JSONL files\n",
    "def train_test_split_jsonl(csv_file_path, train_jsonl_file_path, test_jsonl_file_path):\n",
    "    rows = []\n",
    "    with open(csv_file_path, mode='r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        rows = list(csv_reader)\n",
    "    random.shuffle(rows)\n",
    "    split_index = int(0.8 * len(rows))\n",
    "    train_rows = rows[:split_index]\n",
    "    test_rows = rows[split_index:]\n",
    "    write_to_jsonl(train_jsonl_file_path, train_rows)\n",
    "    write_to_jsonl(test_jsonl_file_path, test_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directory depends on the whether we are working at Google Colab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Repository Detected... \n"
     ]
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Converting ... \")\n",
    "    train_folder_path = f'./AM201_Dataset/AM201_analytical.csv' \n",
    "    train_output_file = f'./examples/data/AM201_train.jsonl'  \n",
    "    test_output_file = f'./examples/data/AM201_test.jsonl'  \n",
    "    train_test_split_jsonl(csv_folder_path, train_output_file, test_output_file)\n",
    "else:\n",
    "    print(\"Local Repository Detected... \")\n",
    "    csv_folder_path = f'../AM201_Dataset/AM201_analytical.csv' \n",
    "    train_output_file = f'./data/AM201_train.jsonl'  \n",
    "    test_output_file = f'./data/AM201_test.jsonl'  \n",
    "    train_test_split_jsonl(csv_folder_path, train_output_file, test_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up API for LLMs (focus on OpenAI models for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the models provided by OpenAI, we first need to set up access to the OpenAI API. For this step, we can follow the [instructions provided by OpenAI](https://platform.openai.com/docs/quickstart?context=python). Test that the API is working by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your API Key Here\n",
    "api_key = \"YOUR_API_KEY\"\n",
    "\n",
    "# You can also set your API key via the OPENAI_API_KEY environment variable\n",
    "#load_dotenv()\n",
    "#openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "# Test the API\n",
    "client = openai.Client(api_key = api_key)\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Customized Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below extracts the solutions from the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Directory Based on Local or Colab\n",
    "\n",
    "if IN_COLAB:\n",
    "    %cd examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for extracting boxed value\n",
    "def extract_number_from_latex(paragraph):\n",
    "    \"\"\"\n",
    "    Extracts a number from a LaTeX paragraph enclosed in the '$\\boxed{}$' pattern.\n",
    "    \n",
    "    Parameters:\n",
    "        paragraph (str): The LaTeX paragraph containing the number enclosed in the '$\\boxed{}$' pattern.\n",
    "        \n",
    "    Returns:\n",
    "        int or None: The extracted number if found, otherwise None.\n",
    "    \"\"\"\n",
    "    # Regular expression to find the $\\boxed{x}$ pattern\n",
    "    # This assumes 'x' is an integer; modify the regex if 'x' can be a different type of number\n",
    "    match = re.search(r'\\$\\\\boxed\\{(\\d+)\\}', paragraph)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calls OpenAI API to generate solutions for the problems in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def math_openai_call(model, jsonl_line):\n",
    "    \"\"\"\n",
    "    Given a model and a line from the customized dataset, output side-by-side the model solution and the target solution to the nth example.\n",
    "    \n",
    "    Parameters:\n",
    "        n (int): n-th line of the .jsonl file.\n",
    "        model (str): a version of chatgpt model.\n",
    "        jsonl_line (str): a line from the customized dataset.\n",
    "        \n",
    "    Returns:\n",
    "        json_data (dict): The original data from the jsonl_line with the added AI solution.\n",
    "    \"\"\"\n",
    "    json_data = json.loads(jsonl_line)\n",
    "    problem = json_data['problem']\n",
    "    solution = json_data['solution']\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant, skilled in solving mathematics problems. Solve this math problem by the user and return the answer (a number) in latex box format, e.g. $\\boxed$:\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{problem}\"},\n",
    "        ],\n",
    "        temperature=1,\n",
    "        max_tokens=256,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    ai_solution = response.choices[0].message.content\n",
    "    json_data['ai_solution'] = ai_solution\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the generated solutions with the ground truth solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for comparing values\n",
    "def compared_boxed_val(paragraph1, paragraph2):\n",
    "    \"\"\"\n",
    "    Given 2 paragraphs, return whether they contain the same boxed value. This function is used to check the ai solution against target solution.\n",
    "    \n",
    "    Parameters:\n",
    "        paragraph1 (str)\n",
    "        paragraph2 (str)\n",
    "        \n",
    "    Returns:\n",
    "        Boolean: True if two paragraphs contain the same boxed value and False otherwise\n",
    "    \"\"\"\n",
    "    num1 = extract_number_from_latex(paragraph1)\n",
    "    num2 = extract_number_from_latex(paragraph2)\n",
    "    return num1 == num2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to test a batch of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def eval_math_problems(input_jsonl_path, output_jsonl_path, model, num_lines_to_parse):\n",
    "    \"\"\"\n",
    "    Test a batch of math problems using the given model.\n",
    "\n",
    "    Args:\n",
    "        input_jsonl_path (str): The path to the input JSONL file containing math problems.\n",
    "        output_jsonl_path (str): The path to the output JSONL file to store the results.\n",
    "        model: The model used to solve the math problems.\n",
    "        num_lines_to_parse (int): The number of lines to parse from the input file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the test score (number of correct answers) and the total count of problems tested.\n",
    "    \"\"\"\n",
    "    test_score = 0\n",
    "    total_count = 0\n",
    "\n",
    "    # Check if output file exists, if not, create it\n",
    "    if not os.path.isfile(output_jsonl_path):\n",
    "        open(output_jsonl_path, 'w').close()\n",
    "\n",
    "    with open(input_jsonl_path, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            # Find which line we are at with output_jsonl_path:\n",
    "            checkpoint = 0\n",
    "            with open(output_jsonl_path, 'r') as outfile:\n",
    "                for j, _ in enumerate(outfile):  # Changed variable name here\n",
    "                    checkpoint += 1\n",
    "            if i < num_lines_to_parse and i >= checkpoint:\n",
    "                json_data = math_openai_call(model, line)\n",
    "                ai_solution = json_data['ai_solution']\n",
    "                solution = json_data['solution']\n",
    "                correct = compared_boxed_val(solution, ai_solution)\n",
    "                json_data['correct'] = correct\n",
    "                if correct:\n",
    "                    test_score += 1\n",
    "                total_count += 1 \n",
    "                with open(output_jsonl_path, 'a') as outfile:\n",
    "                    json.dump(json_data, outfile)\n",
    "                    outfile.write('\\n')\n",
    "                print(f\"Line {i+1} completed.\")\n",
    "            else:\n",
    "                if total_count != 0:\n",
    "                    print(f\"Test score: {test_score/total_count}\")\n",
    "                break\n",
    "    return test_score, total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "eval_math_problems('./data/algebra_train.jsonl', 'algebra_train_result.jsonl', \"gpt-3.5-turbo-1106\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning with Generated Dataaset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the OpenAI API to fine tune GPT-3.5 with the generated dataset. To do so, we first convert the dataset into a .jsonl file format accepted by the OpenAI API. The function below converts the dataset into the system, user, and assistant format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_format(jsonl_line):\n",
    "    \"\"\"\n",
    "    Given a line from the customized dataset, convert it to the OpenAI Chat format.\n",
    "    \n",
    "    Parameters:\n",
    "        jsonl_line (str): a line from the customized dataset.\n",
    "        \n",
    "    Returns:\n",
    "        new_format (str): The converted line in the OpenAI Chat format.\n",
    "    \"\"\"\n",
    "    json_data = json.loads(jsonl_line)\n",
    "    problem = json_data['problem']\n",
    "    solution = json_data['solution']\n",
    "    \n",
    "    new_format = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant, skilled in solving mathematics problems. Solve this math problem by the user and return the answer (a number) in latex box format, e.g. $\\\\boxed$:\"},\n",
    "            {\"role\": \"user\", \"content\": problem},\n",
    "            {\"role\": \"assistant\", \"content\": solution}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return json.dumps(new_format)\n",
    "\n",
    "def convert_file(input_filename, output_filename):\n",
    "    with open(input_filename, 'r') as input_file, open(output_filename, 'w') as output_file:\n",
    "        for line in input_file:\n",
    "            converted_line = convert_format(line)\n",
    "            output_file.write(converted_line + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the dataset into the system, user, and assistant format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_file('./data/algebra_train.jsonl', './data/algebra_train_openai.jsonl')\n",
    "convert_file('./data/algebra_test.jsonl', './data/algebra_test_openai.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the train and test dataset to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_create = client.files.create(\n",
    "  file=open(\"./data/algebra_train_openai.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "val_create = client.files.create(\n",
    "  file=open(\"./data/algebra_test_openai.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get ID of training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('file-MnyaGU86dQGlERAequXA3yHl', 'file-R21MMQOlvPz1LXWqcRMX0OaO')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_file = train_create.id\n",
    "val_file = val_create.id\n",
    "training_file, val_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the fine-tuning job. Now, go to the [OpenAI fine-tuning page](https://platform.openai.com/finetune/), and you should see the fine-tuning job running. After the fine tuning has been completed, you can copy the name of the fine tuned model, which will be used in the `model` parameter in the evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-94ECAxMcgCiNTl16Ao1k6un4', created_at=1702848382, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-1106', object='fine_tuning.job', organization_id='org-Lepq4jszFhmPPFz2pwTV628o', result_files=[], status='validating_files', trained_tokens=None, training_file='file-MnyaGU86dQGlERAequXA3yHl', validation_file=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_create = client.fine_tuning.jobs.create(\n",
    "  training_file=training_file, \n",
    "  model=\"gpt-3.5-turbo-1106\",\n",
    "  epochs=3,\n",
    "  # validation_file=val_file,\n",
    ")\n",
    "ft_create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ftjob-94ECAxMcgCiNTl16Ao1k6un4'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_job = ft_create.id\n",
    "ft_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Performance of Fine-Tuned Model with Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1 completed.\n",
      "Test score: 1.0\n",
      "Original Train Result: (1, 1)\n",
      "Original Test Result: (0, 0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Create a folder called \"results\"\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "def calculate_accuracy(result):\n",
    "    a, b = result\n",
    "    return a / b if b != 0 else 0\n",
    "\n",
    "\n",
    "# Original Model Result\n",
    "train_original_result = eval_math_problems('./data/algebra_train.jsonl', './results/algebra_train_result.jsonl', \"gpt-3.5-turbo-1106\", 1)\n",
    "test_original_result = eval_math_problems('./data/algebra_test.jsonl', './results/algebra_test_result.jsonl', \"gpt-3.5-turbo-1106\", 1)\n",
    "\n",
    "print(f\"Original Train Result: {calculate_accuracy(train_original_result)}\")\n",
    "print(f\"Original Test Result: {calculate_accuracy(test_original_result)}\")\n",
    "\n",
    "# Fine-tuned Model Result\n",
    "train_ft_result = eval_math_problems('./data/algebra_train.jsonl', './results/algebra_train_ft_result.jsonl', ft_job, 1)\n",
    "test_ft_result = eval_math_problems('./data/algebra_test.jsonl', './results/algebra_test_ft_result.jsonl', ft_job, 1)\n",
    "\n",
    "print(f\"Fine-tuned Train Result: {calculate_accuracy(train_ft_result)}\")\n",
    "print(f\"Fine-tuned Test Result: {calculate_accuracy(test_ft_result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
