{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the growing interest in LLMs, there are many work evaluating and fine tuning LLMs on domain specific datasets. One interesting direction is to assess LLMs' ability to perform various forms of mathmetical inferences. These studies could provide an important metric to understand LLMs' fundamental reasoning and cognitive abilities. Linking the broader context to our course, we are interested in systematically assessing the reasoning ability of different LLMs on some fundamental physical mathmetics problems, and creating a pipeline for fine tuning the LLMs on these domain specific datasets. We think this would be a relevant research direction for the following reasons: \n",
    "- The nature of these physical mathmetics problems particularly emphasizes intuitive approximations and abstraction abilities, which is known to be a weak point of many LLMs ([Mishra, Swaroop et al.](https://arxiv.org/abs/2204.05660))\n",
    "- Through conducting a questionnaire-based study across the semester, we as a whole class got anecdotal evidence suggesting that different LLMs in general perform not very satisfying on doing these problems.\n",
    "\n",
    "Therefore, these reasons motivate us to develop a structured framework for quantitatively evaluate and define the model performance on the specific problems of interests, and to improve the model performance through fine tuning. Moreover, we provided instructions and fucntions to streamline different steps of the whole process, so that this can be used as a platform solution for many similar problems. In this notebook, we will present the process in the format of a tutorial, using a standard open source math dataset (github link) as illustration. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Customized Dataset Generation\n",
    "    - instructions on data content preparation\n",
    "    - data formatting\n",
    "- ### Setting up API for LLMs (focus on OpenAI models for now)\n",
    "- ### Evaluation of Customized Dataset\n",
    "    - exploratory testing and visulization\n",
    "    - batch testing to produce accuracy metrics\n",
    "- ### Fine Tuning (with Axolotl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package imports\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: instructions on data content preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions:\n",
    "A problem should contain the following fields:\n",
    "- problem statement\n",
    "- category label (e.g. AM201)\n",
    "- difficulty level (optional)\n",
    "- solution \n",
    "#### Format: \n",
    "The data content can be prepared either in a .cvs format or .json/.jsonl format. Here is a [Google sheet example](https://docs.google.com/spreadsheets/d/18NZErrCDF55uFSx_AjWUba93NSVDs4yHGcM9ZxiFRf4/edit#gid=0) for the .csv format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: data formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function combines individual .json files into a .jsonl file (standard formatting used for inference and fine tuning). The cell below uses the algebra problems in Math Dataset as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_json_to_jsonl(folder_path, output_file):\n",
    "    json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for file_name in json_files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            with open(file_path, 'r') as infile:\n",
    "                data = json.load(infile)\n",
    "                json.dump(data, outfile)\n",
    "                outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "folder_path = 'MATH/test/algebra/'  # Replace with the path to your folder\n",
    "output_file = 'algebra.jsonl'         # Name of the output .jsonl file\n",
    "concatenate_json_to_jsonl(folder_path, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up API for LLMs (focus on OpenAI models for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the models provided by OpenAI, we first need to set up access to the OpenAI API. For this step, we can follow the [instructions provided by OpenAI](https://platform.openai.com/docs/quickstart?context=python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"In the realm where programs dance and play,\\nWhere algorithms rule, night and day,\\nLies a concept both strange and bold,\\nA pattern that programmers behold.\\n\\nGather, my friends, listen with delight,\\nFor I shall unveil recursion's true might.\\nImagine a function, a magical spell,\\nThat calls itself, as if under a spell.\\n\\nThrough the abyss of repetition it dives,\\nLike echoes in the forest that connive,\\nA journey through the looking glass,\\nTo unravel mysteries that surpass.\\n\\nPicture a labyrinth, twisted and strong,\\nWhere paths intertwine, right and wrong,\\nWith each step taken, a new world unfolds,\\nAs recursion's power gently behold.\\n\\nWithin the labyrinth, a problem resides,\\nAwaiting a hero who in code abides.\\nTo solve it, we break it down, you see,\\nInto smaller versions, with simplicity.\\n\\nAs the function calls itself once more,\\nIt dives deeper, exploring at its core,\\nThe problem reduced, with each recursion,\\nUntil a base case ends the mission.\\n\\nLike a fractal fern,  endlessly detailed,\\nRecursion weaves patterns, perfectly veiled.\\nWith elegance and grace, it crafts anew,\\nSolutions emerge, in crystal clear view.\\n\\nBut heed my words, oh programmers dear,\\nRecursion wields power, but also fear.\\nWith infinite loops, if not controlled,\\nYour program's fate may soon be told.\\n\\nSo embrace this concept, with due respect,\\nA double-edged sword, you must protect.\\nHarness its magic, explore its domain,\\nRecursion, a tool that does not wane.\\n\\nIn the land of code, where wonders unfold,\\nRecursion's mystery remains untold.\\nA journey of depth, where beauty resides,\\nIn this dance between function and its bride.\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = openai.Client()\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Customized Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: exploratory testing and visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem: Factor $-16x^4+x^2+2x+1$ into two quadratic polynomials with integer coefficients.  Submit your answer in the form $(ax^2+bx+c)(dx^2+ex+f)$, with $a<d$.\n",
      "--------------------------------------------------\n",
      "AI Solution: We can factor the given expression by grouping the terms:\n",
      "\n",
      "oxed$: $(-4x^2+1)(4x^2-1)$\n",
      "Actual Solution: Note that $-16x^4+x^2+2x+1=(x+1)^2-(4x^2)^2=\\boxed{(-4x^2+x+1)(4x^2+x+1)}$, where we have used the difference of squares identity for the second equality.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def test_math_problems_n(n, model, jsonl_path):\n",
    "    \"\"\"\n",
    "    Given a model and a file path to the customized dataset, output side-by-side the model solution and the target solution to the nth example.\n",
    "    \n",
    "    Parameters:\n",
    "        n (int): n-th line of the .jsonl file.\n",
    "        model (str): a version of chatgpt model.\n",
    "        jsonl_path (str): path to the customized dataset.\n",
    "        \n",
    "    Returns:\n",
    "        ai solution (str): model solution.\n",
    "        target solution (str): target solution.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(jsonl_path):\n",
    "            print(f\"File {file_path} not found.\")\n",
    "    with open(jsonl_path, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            checkpoint = 0\n",
    "            if i == n:\n",
    "                json_data = json.loads(line)\n",
    "                problem = json_data['problem']\n",
    "                solution = json_data['solution']\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant, skilled in solving common algebraic problems.\"},\n",
    "                        {\"role\": \"user\", \"content\": f\"Solve this math problem and return the answer (a number) in latex box format, e.g. $\\boxed$: {problem}\"},\n",
    "                    ],\n",
    "                    temperature=1,\n",
    "                    max_tokens=256,\n",
    "                    top_p=1,\n",
    "                    frequency_penalty=0,\n",
    "                    presence_penalty=0\n",
    "                )\n",
    "                ai_solution = response.choices[0].message.content\n",
    "                print(f\"Problem: {problem}\")\n",
    "                print(\"--------------------------------------------------\")\n",
    "                print(f\"AI Solution: {ai_solution}\")\n",
    "                print(f\"Actual Solution: {solution}\")\n",
    "                print(\"--------------------------------------------------\")\n",
    "    return ai_solution, solution\n",
    "\n",
    "ai_solution, target_solution = test_math_problems_n(1, \"gpt-3.5-turbo-1106\", 'algebra.jsonl')  # Change the number as per your requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for extracting boxed value\n",
    "def extract_number_from_latex(paragraph):\n",
    "    \"\"\"\n",
    "    Extracts a number from a LaTeX paragraph enclosed in the '$\\boxed{}$' pattern.\n",
    "    \n",
    "    Parameters:\n",
    "        paragraph (str): The LaTeX paragraph containing the number enclosed in the '$\\boxed{}$' pattern.\n",
    "        \n",
    "    Returns:\n",
    "        int or None: The extracted number if found, otherwise None.\n",
    "    \"\"\"\n",
    "    # Regular expression to find the $\\boxed{x}$ pattern\n",
    "    # This assumes 'x' is an integer; modify the regex if 'x' can be a different type of number\n",
    "    match = re.search(r'\\$\\\\boxed\\{(\\d+)\\}', paragraph)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for comparing values\n",
    "def compared_boxed_val(paragraph1, paragraph2):\n",
    "    \"\"\"\n",
    "    Given 2 paragraphs, return whether they contain the same boxed value. This function is used to check the ai solution against target solution.\n",
    "    \n",
    "    Parameters:\n",
    "        paragraph1 (str)\n",
    "        paragraph2 (str)\n",
    "        \n",
    "    Returns:\n",
    "        Boolean: 1 if two paragraphs contain the same boxed value and 0 otherwise\n",
    "    \"\"\"\n",
    "    num1 = extract_number_from_latex(paragraph1)\n",
    "    num2 = extract_number_from_latex(paragraph2)\n",
    "    return 1*(num1 == num2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usage\n",
    "compared_boxed_val(target_solution, ai_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: batch testing to produce accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_math_problems_batch(jsonl_path, model, num_lines_to_parse,output_jsonl_path):\n",
    "    \"\"\"\n",
    "    Test a batch of math problems using the given model.\n",
    "\n",
    "    Parameters:\n",
    "    - jsonl_path (str): The path to the JSONL file containing the math problems.\n",
    "    - model (str): The name of the model to use for solving the problems.\n",
    "    - num_lines_to_parse (int): The number of lines to parse from the JSONL file.\n",
    "    - output_jsonl_path (str): The path to the output JSONL file to store the results.\n",
    "\n",
    "    Returns:\n",
    "    - test_score (int): The total score obtained from solving the math problems.\n",
    "    - average_score (float): The average score obtained from solving the math problems.\n",
    "    \"\"\"\n",
    "    test_score = 0\n",
    "    total_count = 0\n",
    "    with open(jsonl_path, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            # Find which line we are at with output_jsonl_path:\n",
    "            checkpoint = 0\n",
    "            with open(output_jsonl_path, 'r') as outfile:\n",
    "                for j, line in enumerate(outfile):\n",
    "                    checkpoint += 1\n",
    "            if i < num_lines_to_parse and i >= checkpoint:\n",
    "                json_data = json.loads(line)\n",
    "                problem = json_data['problem']\n",
    "                solution = json_data['solution']\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are an assistant skilled in solving common algebraic problems. Solve this math problem given by the user and return the answer (a number) in latex box format, e.g. $\\boxed$:\"},\n",
    "                        {\"role\": \"user\", \"content\": f\" {problem}\"},\n",
    "                    ],\n",
    "                    temperature=1,\n",
    "                    max_tokens=256,\n",
    "                    top_p=1,\n",
    "                    frequency_penalty=0,\n",
    "                    presence_penalty=0\n",
    "                )\n",
    "                time.sleep(5)\n",
    "                ai_solution = response.choices[0].message.content\n",
    "                score = compared_boxed_val(solution, ai_solution)\n",
    "                test_score += score\n",
    "                total_count += 1\n",
    "                result = {\n",
    "                    \"problem\": problem,\n",
    "                    \"target_solution\": solution,\n",
    "                    \"gpt_solution\": ai_solution,\n",
    "                    \"score\": score\n",
    "                }\n",
    "                with open(output_jsonl_path, 'a') as outfile:\n",
    "                    json.dump(result, outfile)\n",
    "                    outfile.write('\\n')\n",
    "            else:\n",
    "                break\n",
    "    return test_score, test_score/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 0.8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_math_problems_batch('algebra.jsonl', \"gpt-3.5-turbo-1106\", 5, 'algebra_test_sol.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
